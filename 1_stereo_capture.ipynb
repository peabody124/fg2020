{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibration and pose extraction testing\n",
    "\n",
    "1. record video with calibration marker (checkerboard grid)\n",
    "2. extract checkerboard and calibrate cameras\n",
    "3. record video with stereo video of movement\n",
    "4. extract pose from each camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synchronized_recording(output_video_file, size=(640, 480), fps=30):\n",
    "    \"\"\" Acquire data from two webcams as synchronously as possible\n",
    "    \n",
    "        Creates a video file with the two videos vertically concatenated.\n",
    "    \"\"\"\n",
    "    import cv2\n",
    "    import numpy as np\n",
    "    import time\n",
    "    \n",
    "    win, hin = size\n",
    "    \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    out = cv2.VideoWriter(output_video_file, fourcc, fps, (int(win), int(hin * 2)))\n",
    "\n",
    "    # open camera\n",
    "    cam1 = cv2.VideoCapture(0)\n",
    "    cam2 = cv2.VideoCapture(1)\n",
    "\n",
    "    cam1.set(cv2.CAP_PROP_AUTOFOCUS, 0)\n",
    "    cam1.set(cv2.CAP_PROP_FPS, fps)\n",
    "    cam1.set(cv2.CAP_PROP_FRAME_WIDTH, int(win))\n",
    "    cam1.set(cv2.CAP_PROP_FRAME_HEIGHT, int(hin))\n",
    "\n",
    "    cam2.set(cv2.CAP_PROP_AUTOFOCUS, 0)    \n",
    "    cam2.set(cv2.CAP_PROP_FPS, fps)\n",
    "    cam2.set(cv2.CAP_PROP_FRAME_WIDTH, int(win))\n",
    "    cam2.set(cv2.CAP_PROP_FRAME_HEIGHT, int(hin))\n",
    "    \n",
    "    timestamps = []\n",
    "\n",
    "    while True:\n",
    "\n",
    "        ret1 = cam1.grab()\n",
    "        ret2 = cam2.grab()\n",
    "\n",
    "        if not ret1 or not ret2:\n",
    "            print(\"Something is wrong with camera\")\n",
    "            break\n",
    "\n",
    "        ret1, img1 = cam1.retrieve()\n",
    "        ret2, img2 = cam2.retrieve()\n",
    "        \n",
    "        if not ret1 or img1 is None:\n",
    "            print('Missed img1')\n",
    "            continue\n",
    "        if not ret2 or img2 is None:\n",
    "            print('Missed img2')\n",
    "            continue\n",
    "            \n",
    "        timestamps.append(time.time())\n",
    "        combined = np.concatenate((img1, img2), axis=0)\n",
    "\n",
    "        out.write(combined)\n",
    "        \n",
    "        cv2.imshow('combined', combined)\n",
    "        key = cv2.waitKey(1)\n",
    "        if key == ord('q'):\n",
    "            break                \n",
    "            \n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    return timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_stereo_video(filename, callback):\n",
    "    \"\"\" Process a stereo video\n",
    "    \n",
    "        Expects the format to match that from synchronized_recording and\n",
    "        pass the top and bottom parts to the callback to analyze. The\n",
    "        callback is expected to handle its own results.\n",
    "    \"\"\"\n",
    "    import cv2\n",
    "    import numpy as np\n",
    "    \n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    vid = cv2.VideoCapture(filename)\n",
    "\n",
    "    win = vid.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "    hin = vid.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "    fps = vid.get(cv2.CAP_PROP_FPS)\n",
    "    frames = int(vid.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    for i in tqdm(range(frames)):\n",
    "        ret, im = vid.read()\n",
    "        top, bottom = np.split(im, 2, axis=0)\n",
    "        \n",
    "        ret = callback(top, bottom, i)\n",
    "        if ret is False:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibrate stereo cameras\n",
    "\n",
    "Obtain video of a checkerboard from two cameras and compute the intrinsic and extrinsic camera properties. When running the calibration routine, ensure that the checkboard vertices are well localized or the calibration will be poor. If needed, print a much larger grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = synchronized_recording('checkerboard_calibration.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now detect the checkerboard in all frames\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# termination criteria\n",
    "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 3, 0.001)\n",
    "\n",
    "# NOTE: change the spacing below for a different grid size\n",
    "# prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)\n",
    "objp = np.zeros((6*8,3), np.float32)\n",
    "objp[:,:2] = np.mgrid[0:8,0:6].T.reshape(-1,2) * 2.3\n",
    "\n",
    "# Arrays to store object points and image points from all the images.\n",
    "objpoints = [] # 3d point in real world space\n",
    "im1points = [] # 2d points in image plane.\n",
    "im2points = [] # 2d points in image plane.\n",
    "\n",
    "def get_checkerboard(im):\n",
    "    # Find the chess board corners\n",
    "    gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "    ret, corners = cv2.findChessboardCorners(gray, (8, 6), None)\n",
    "    if ret:\n",
    "        return cv2.cornerSubPix(gray, corners, (11,11), (-1,-1), criteria)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def frame_checkboards(left, right, i, plot=True):\n",
    "    c1 = get_checkerboard(left)\n",
    "    c2 = get_checkerboard(right)\n",
    "    \n",
    "    if i % 10 != 0: return True\n",
    "    \n",
    "    if c1 is not None and c2 is not None:\n",
    "        objpoints.append(objp)\n",
    "        im1points.append(c1)\n",
    "        im2points.append(c2)\n",
    "        \n",
    "        if plot:\n",
    "            o1 = cv2.drawChessboardCorners(left, (8,6), c1, True)\n",
    "            o2 = cv2.drawChessboardCorners(right, (8,6), c2, True)\n",
    "                \n",
    "            combined = np.concatenate((o1, o2), axis=0)\n",
    "            cv2.imshow('combined', combined)\n",
    "            if cv2.waitKey(1) == ord('q'): return False\n",
    "            \n",
    "    return len(objpoints) < 100\n",
    "\n",
    "parse_stereo_video('checkerboard_calibration.mp4', frame_checkboards)\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "len(objpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# independently calibrate each camera based on the \n",
    "\n",
    "def calibrate(objpoints, imgpoints):\n",
    "    h, w, d = (640, 480, 3)\n",
    "    ret, mtx, dist, _, _ = cv2.calibrateCamera(objpoints, imgpoints, (w, h), None, None)\n",
    "    newcameramtx, roi = cv2.getOptimalNewCameraMatrix(mtx, dist, (w, h), 1, (w, h))\n",
    "    return {'mtx': mtx, 'dist': dist, 'newcameramtx': newcameramtx, 'roi': roi}\n",
    "\n",
    "if True:\n",
    "    print('Calibrating')\n",
    "    calibration1 = calibrate(objpoints[::1], im1points[::1])\n",
    "    calibration2 = calibrate(objpoints[::1], im2points[::1])\n",
    "\n",
    "    print(calibration1)\n",
    "    print(calibration2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now perform a stereo calibration which refines the intrinsics (optional)\n",
    "# and computes the extrinsic calibration\n",
    "\n",
    "h, w, d = (640, 480, 3)\n",
    "stereocalib_criteria = (cv2.TERM_CRITERIA_EPS, 100, 1e-5)\n",
    "\n",
    "# note there are many components of the camera view that can be calibrated. In this case using\n",
    "# the initial guess from the prior step and forcing the same focal length as I am using identical\n",
    "# cameras\n",
    "\n",
    "#stereocalib_flags = 0\n",
    "#stereocalib_flags = cv2.CALIB_FIX_INTRINSIC\n",
    "#stereocalib_flags = cv2.CALIB_SAME_FOCAL_LENGTH\n",
    "#stereocalib_flags = cv2.CALIB_FIX_ASPECT_RATIO | cv2.CALIB_SAME_FOCAL_LENGTH\n",
    "#stereocalib_flags = cv2.CALIB_SAME_FOCAL_LENGTH | cv2.CALIB_FIX_INTRINSIC | cv2.CALIB_ZERO_TANGENT_DIST #cv2.CALIB_ZERO_TANGENT_DIST | cv2.CALIB_SAME_FOCAL_LENGTH | cv2.CALIB_RATIONAL_MODEL | cv2.CALIB_FIX_K3 | cv2.CALIB_FIX_K4 | cv2.CALIB_FIX_K5\n",
    "#stereocalib_flags = cv2.CALIB_SAME_FOCAL_LENGTH | cv2.CALIB_ZERO_TANGENT_DIST | cv2.CALIB_SAME_FOCAL_LENGTH | cv2.CALIB_RATIONAL_MODEL | cv2.CALIB_FIX_K3 | cv2.CALIB_FIX_K4 | cv2.CALIB_FIX_K5\n",
    "#stereocalib_flags = cv2.CALIB_SAME_FOCAL_LENGTH | cv2.CALIB_FIX_K1 | cv2.CALIB_FIX_K2 | cv2.CALIB_FIX_K3 | cv2.CALIB_ZERO_TANGENT_DIST\n",
    "stereocalib_flags = cv2.CALIB_USE_INTRINSIC_GUESS | cv2.CALIB_SAME_FOCAL_LENGTH # actually this worked  great!\n",
    "\n",
    "\n",
    "res = cv2.stereoCalibrate(objpoints, im1points, im2points,\n",
    "                          calibration1['mtx'].copy(), calibration1['dist'].copy() * 0,\n",
    "                          calibration2['mtx'].copy(), calibration2['dist'].copy() * 0,\n",
    "                          (w, h),                          \n",
    "                          criteria = stereocalib_criteria,\n",
    "                          flags = stereocalib_flags)\n",
    "stereocalib_retval, cameraMatrix1, distCoeffs1, cameraMatrix2, distCoeffs2, R, T, E, F = res\n",
    "\n",
    "data = {'cameraMatrix1': cameraMatrix1, 'distCoeffs1': distCoeffs1,\n",
    "        'cameraMatrix2': cameraMatrix2, 'distCoeffs2': distCoeffs2,\n",
    "        'R': R, 'T': T, 'E': E, 'F': F}\n",
    "\n",
    "print(stereocalib_retval)\n",
    "[print(k, '\\n', data[k]) for k in data.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rectify_scale = 1.0 # 0=full crop, 1=no crop\n",
    "\n",
    "R1, R2, P1, P2, Q, roi1, roi2 = cv2.stereoRectify(data[\"cameraMatrix1\"], data[\"distCoeffs1\"],\n",
    "                                                  data[\"cameraMatrix2\"], data[\"distCoeffs2\"],\n",
    "                                                  (w, h), data[\"R\"], data[\"T\"], alpha=rectify_scale)\n",
    "#, flags=0*cv2.CALIB_ZERO_DISPARITY,\n",
    "#                                                  alpha = rectify_scale)\n",
    "\n",
    "data['R1'] = R1\n",
    "data['R2'] = R2\n",
    "data['P1'] = P1\n",
    "data['P2'] = P2\n",
    "data['Q'] = Q\n",
    "data['roi1'] = roi1\n",
    "data['roi2'] = roi2\n",
    "\n",
    "print(roi1)\n",
    "print(roi2)\n",
    "print(P1)\n",
    "print(P2)\n",
    "\n",
    "left_maps = cv2.initUndistortRectifyMap(data[\"cameraMatrix1\"], data[\"distCoeffs1\"], R1, P1, (w, h), cv2.CV_16SC2)\n",
    "right_maps = cv2.initUndistortRectifyMap(data[\"cameraMatrix2\"], data[\"distCoeffs2\"], R2, P2, (w, h), cv2.CV_16SC2)\n",
    "\n",
    "def play_rectified(left, right, i):\n",
    "    left_img_remap = cv2.remap(left, left_maps[0], left_maps[1], cv2.INTER_LANCZOS4)\n",
    "    right_img_remap = cv2.remap(right, right_maps[0], right_maps[1], cv2.INTER_LANCZOS4) #cv2.INTER_LANCZOS4)\n",
    "    \n",
    "    #combined = left_img_remap\n",
    "    combined = np.concatenate((left_img_remap, right_img_remap), axis=0)\n",
    "    #combined = np.concatenate((left, right), axis=0)\n",
    "\n",
    "    cv2.imshow('combined', combined)\n",
    "\n",
    "    key = cv2.waitKey(1)\n",
    "    return key != ord('q')       \n",
    "\n",
    "parse_stereo_video('checkerboard_calibration.mp4', play_rectified)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same the calibration data, which will be used in the fusion step\n",
    "\n",
    "with open('calibration.pkl', 'wb') as handle:\n",
    "    pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Record movement video for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: You should be acquiring inertial data at the same time\n",
    "\n",
    "timestamps = synchronized_recording('movement.mp4')\n",
    "np.save('movement_timestamps', timestamps) # make backup in case of crash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process video with OpenPose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: you must compile openpose with the python bindings and make\n",
    "# sure that jupyter can find it for the following code\n",
    "\n",
    "home = os.path.expanduser(\"~\")\n",
    "openpose_python_path = os.path.join(home, 'projects/pose/openpose/build/python')\n",
    "\n",
    "sys.path.append(openpose_python_path)\n",
    "\n",
    "from openpose import pyopenpose as op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_openpose_parser(results_path):\n",
    "    params = {'model_folder': os.path.join(openpose_python_path, '../../models'),\n",
    "              'number_people_max': 1, 'write_json': results_path}\n",
    "\n",
    "    #params[\"body\"] = 1\n",
    "\n",
    "    #params[\"face\"] = False\n",
    "    #params[\"face_detector\"] = 2\n",
    "\n",
    "    #params[\"hand\"] = False\n",
    "    #params[\"hand_detector\"] = 2\n",
    "\n",
    "    opWrapper = op.WrapperPython()\n",
    "    opWrapper.configure(params)\n",
    "    opWrapper.start()\n",
    "    \n",
    "    return opWrapper\n",
    "\n",
    "faceRectangles = [\n",
    "    op.Rectangle(330.119385, 277.532715, 48.717274, 48.717274),\n",
    "    op.Rectangle(24.036991, 267.918793, 65.175171, 65.175171),\n",
    "    op.Rectangle(151.803436, 32.477852, 108.295761, 108.295761),\n",
    "]\n",
    "\n",
    "handRectangles = [\n",
    "    # Left/Right hands person 0\n",
    "    [\n",
    "    op.Rectangle(320.035889, 377.675049, 69.300949, 69.300949),\n",
    "    op.Rectangle(0., 0., 0., 0.),\n",
    "    ],\n",
    "    # Left/Right hands person 1\n",
    "    [\n",
    "    op.Rectangle(80.155792, 407.673492, 80.812706, 80.812706),\n",
    "    op.Rectangle(46.449715, 404.559753, 98.898178, 98.898178),\n",
    "    ],\n",
    "    # Left/Right hands person 2\n",
    "    [\n",
    "    op.Rectangle(185.692673, 303.112244, 157.587555, 157.587555),\n",
    "    op.Rectangle(88.984360, 268.866547, 117.818230, 117.818230),\n",
    "    ]\n",
    "]\n",
    "\n",
    "OP_NOSE = 0\n",
    "OP_NECK = 1\n",
    "OP_RSHOULDER = 2\n",
    "OP_RELBOW = 3\n",
    "OP_RWRIST = 4\n",
    "OP_LSHOULDER = 5\n",
    "OP_LELBOW = 6\n",
    "OP_LWRIST = 7\n",
    "OP_MIDHIP = 8\n",
    "OP_RHIP = 9\n",
    "OP_RKNEE = 10\n",
    "OP_RANKLE = 11\n",
    "OP_LHIP = 12\n",
    "OP_LKNEE = 13\n",
    "OP_LANKLE = 14\n",
    "OP_REYE = 15\n",
    "OP_LEYE = 16\n",
    "OP_LBIGTOE = 19\n",
    "OP_LSMALLTOE = 20\n",
    "OP_LHEEL = 21\n",
    "OP_RBIGTOE = 22\n",
    "OP_RSMALLTOE = 23\n",
    "OP_RHEEL = 24\n",
    "\n",
    "PARTS = [\"Nose\", \"Neck\", \"RShoulder\", \"RElbow\", \"RWrist\", \"LShoulder\", \"LElbow\", \"LWrist\", \"MidHip\", \"RHip\",\n",
    "         \"RKnee\", \"RAnkle\", \"LHip\", \"LKnee\", \"LAnkle\", \"REye\", \"LEye\", \"REar\", \"LEar\", \"LBigToe\", \"LSmallToe\", \n",
    "         \"LHeel\", \"RBigToe\", \"RSmallToe\", \"RHeel\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process video\n",
    "\n",
    "fn = f'movement.mp4'\n",
    "timestamps = np.load(f'movement_timestamps.npy')\n",
    "\n",
    "base = os.path.splitext(fn)[0]\n",
    "out_fn = base + \"_openpose.mp4\"\n",
    "pickle_file = base + \".pkl\"\n",
    "json_path = base + '_json'\n",
    "\n",
    "opWrapper = get_openpose_parser(json_path)\n",
    "import cv2\n",
    "vid = cv2.VideoCapture(fn)\n",
    "\n",
    "win = vid.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "hin = vid.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "fps = vid.get(cv2.CAP_PROP_FPS)\n",
    "total_frames = vid.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter(out_fn, fourcc, fps, (int(win), int(hin)))\n",
    "\n",
    "keypointsLeft = []\n",
    "keypointsRight = []\n",
    "allPoints = []\n",
    "\n",
    "def process_frame(im):\n",
    "    datum = op.Datum()\n",
    "    datum.cvInputData = im\n",
    "    datum.faceRectangles = faceRectangles\n",
    "    datum.handRectangles = handRectangles\n",
    "    opWrapper.emplaceAndPop([datum])\n",
    "    \n",
    "    return datum.cvOutputData, datum.poseKeypoints\n",
    "\n",
    "def frame_pose_extraction(left, right, i, plot=True, save=True):\n",
    "    ol1, kp1 = process_frame(left)\n",
    "    ol2, kp2 = process_frame(right)\n",
    "\n",
    "    keypointsLeft.append(kp1)\n",
    "    keypointsRight.append(kp2)\n",
    "    allPoints.append(np.concatenate((kp1.reshape(-1), kp2.reshape(-1))))\n",
    "    \n",
    "    combined = np.concatenate((ol1, ol2), axis=0)\n",
    "    \n",
    "    if plot:\n",
    "        cv2.imshow('combined', combined)\n",
    "        if cv2.waitKey(1) == ord('q'): return False\n",
    "        \n",
    "    if save:\n",
    "        out.write(combined)\n",
    "            \n",
    "    return True\n",
    "\n",
    "\n",
    "parse_stereo_video(fn, frame_pose_extraction)\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save joint positions from each camera and confidences\n",
    "\n",
    "columns = pd.MultiIndex.from_product([['camL', 'camR'], PARTS, ['x', 'y', 'c']], names=['side', 'part', 'component'])\n",
    "timestamp_index = pd.DatetimeIndex([pd.Timestamp.fromtimestamp(t) for t in timestamps])\n",
    "stereo_keypoints = pd.DataFrame(allPoints, columns=columns, index=timestamp_index)\n",
    "stereo_keypoints.to_pickle(pickle_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triangulate pose into 3D coordinates\n",
    "\n",
    "This is not really used subsequently, but demonstrates how the coordinates can be projected into 3D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401 unused import\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cal = pickle.load(open('calibration.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts = stereo_keypoints.columns.get_level_values(1).unique()\n",
    "p3d = []\n",
    "\n",
    "smoothed = stereo_keypoints.ewm(10).mean()\n",
    "\n",
    "for p in parts:\n",
    "    p1 = smoothed['camL'][p][['y', 'x']].values\n",
    "    p2 = smoothed['camR'][p][['y', 'x']].values\n",
    "        \n",
    "    # compute 3d homogeneous\n",
    "    p3 = cv2.triangulatePoints(cal['P1'], cal['P2'], p1.transpose(), p2.transpose()).transpose()\n",
    "    p3 = cv2.convertPointsFromHomogeneous(p3)[:,0,:]\n",
    "    p3d.append(p3)\n",
    "\n",
    "p3d = np.concatenate(p3d, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = pd.MultiIndex.from_product([parts, ['x', 'y', 'z']], names=['part', 'component'])\n",
    "p3d = pd.DataFrame(p3d, columns=columns, index=stereo_keypoints.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = p3d[['RWrist', 'RKnee']].iloc[150:800].plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
