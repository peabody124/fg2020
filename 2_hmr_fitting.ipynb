{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note this code uses \n",
    "\n",
    "* Python 3.7\n",
    "* Tensorflow 1.14.0\n",
    "* HMR from [here](https://github.com/peabody124/hmr/) with a few compatibility updates\n",
    "\n",
    "Please register and download SMPL from here https://smpl.is.tue.mpg.de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pylab inline\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import skimage.io as io\n",
    "\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the HMR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sys.path.append('../hmr')\n",
    "\n",
    "from src.util import image as img_util\n",
    "from src.util import openpose as op_util\n",
    "import src.config\n",
    "from src.RunModel import RunModel\n",
    "\n",
    "# fake that we are running an app like in hmr/demo.py\n",
    "from absl import flags\n",
    "config = flags.FLAGS\n",
    "config(['hmr.py'])\n",
    "config.load_path = src.config.PRETRAINED_MODEL\n",
    "\n",
    "#config.img_size = 640\n",
    "config.batch_size = 1\n",
    "\n",
    "hmr_graph = tf.get_default_graph()\n",
    "with hmr_graph.as_default():\n",
    "    model = RunModel(config, sess=tf.Session(graph=hmr_graph))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_fn = f'movement.mp4'\n",
    "vid = cv2.VideoCapture(video_fn)\n",
    "total_frames = int(vid.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "print(total_frames)\n",
    "vid.release()\n",
    "\n",
    "def get_frame(frame_id, top=True):\n",
    "    vid = cv2.VideoCapture(video_fn)\n",
    "\n",
    "    vid.set(cv2.CAP_PROP_POS_FRAMES, frame_id)\n",
    "    ret, frame = vid.read()\n",
    "\n",
    "    # only keep one of the two cameras\n",
    "    if top:\n",
    "        frame = frame[:480]\n",
    "    else:\n",
    "        frame = frame[480:]\n",
    "    \n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(img, json_path=None):\n",
    "    \n",
    "    if json_path is None:\n",
    "        if np.max(img.shape[:2]) != config.img_size:\n",
    "            scale = (float(config.img_size) / np.max(img.shape[:2]))\n",
    "        else:\n",
    "            scale = 1.\n",
    "\n",
    "        center = np.round(np.array(img.shape[:2]) / 2).astype(int)\n",
    "        # image center in (x,y)\n",
    "        center = center[::-1]\n",
    "    else:\n",
    "        scale, center = op_util.get_bbox(json_path)\n",
    "    \n",
    "    crop, proc_param = img_util.scale_and_crop(img, scale, center, config.img_size)\n",
    "\n",
    "    # Normalize image to [-1, 1]\n",
    "    crop = 2 * ((crop / 255.) - 0.5)\n",
    "\n",
    "    return crop, proc_param, img\n",
    "\n",
    "def get_json_path(frame_id, top=True):\n",
    "    n = frame_id * 2 + top\n",
    "    return f'movement{filename_N}_json/{n}_keypoints.json'\n",
    "\n",
    "frame_id = 100\n",
    "top = True\n",
    "\n",
    "print(get_json_path(frame_id, top))\n",
    "frame = get_frame(frame_id, top)\n",
    "crop, proc_param, img = preprocess_image(frame, get_json_path(frame_id, top))\n",
    "\n",
    "input_img = np.expand_dims(crop, 0)\n",
    "joints, verts, cams, joints3d, theta = model.predict(input_img, get_theta=True)\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(0.5 + 0.5 * input_img[0])\n",
    "for j in joints[0]:\n",
    "    plt.plot(j[0], j[1], 'ow', MarkerSize=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize with Open3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialization.py\n",
    "\n",
    "import sys\n",
    "sys.path.append('../smpl_modified/smpl_webuser')\n",
    "import numpy as np\n",
    "import _pickle as old_pickle\n",
    "import chumpy as ch\n",
    "from chumpy.ch import MatVecMult\n",
    "from posemapper import posemap\n",
    "from verts import verts_core\n",
    "    \n",
    "def save_model(model, fname):\n",
    "    m0 = model\n",
    "    trainer_dict = {'v_template': np.asarray(m0.v_template),'J': np.asarray(m0.J),'weights': np.asarray(m0.weights),'kintree_table': m0.kintree_table,'f': m0.f, 'bs_type': m0.bs_type, 'posedirs': np.asarray(m0.posedirs)}    \n",
    "    if hasattr(model, 'J_regressor'):\n",
    "        trainer_dict['J_regressor'] = m0.J_regressor\n",
    "    if hasattr(model, 'J_regressor_prior'):\n",
    "        trainer_dict['J_regressor_prior'] = m0.J_regressor_prior\n",
    "    if hasattr(model, 'weights_prior'):\n",
    "        trainer_dict['weights_prior'] = m0.weights_prior\n",
    "    if hasattr(model, 'shapedirs'):\n",
    "        trainer_dict['shapedirs'] = m0.shapedirs\n",
    "    if hasattr(model, 'vert_sym_idxs'):\n",
    "        trainer_dict['vert_sym_idxs'] = m0.vert_sym_idxs\n",
    "    if hasattr(model, 'bs_style'):\n",
    "        trainer_dict['bs_style'] = model.bs_style\n",
    "    else:\n",
    "        trainer_dict['bs_style'] = 'lbs'\n",
    "    pickle.dump(trainer_dict, open(fname, 'w'), -1)\n",
    "\n",
    "\n",
    "def backwards_compatibility_replacements(dd):\n",
    "\n",
    "    # replacements\n",
    "    if 'default_v' in dd:\n",
    "        dd['v_template'] = dd['default_v']\n",
    "        del dd['default_v']\n",
    "    if 'template_v' in dd:\n",
    "        dd['v_template'] = dd['template_v']\n",
    "        del dd['template_v']\n",
    "    if 'joint_regressor' in dd:\n",
    "        dd['J_regressor'] = dd['joint_regressor']\n",
    "        del dd['joint_regressor']\n",
    "    if 'blendshapes' in dd:\n",
    "        dd['posedirs'] = dd['blendshapes']\n",
    "        del dd['blendshapes']\n",
    "    if 'J' not in dd:\n",
    "        dd['J'] = dd['joints']\n",
    "        del dd['joints']\n",
    "\n",
    "    # defaults\n",
    "    if 'bs_style' not in dd:\n",
    "        dd['bs_style'] = 'lbs'\n",
    "\n",
    "\n",
    "\n",
    "def ready_arguments(fname_or_dict):\n",
    "\n",
    "    if not isinstance(fname_or_dict, dict):\n",
    "        dd = old_pickle.load(open(fname_or_dict, 'rb'), fix_imports=True, encoding='latin1')\n",
    "    else:\n",
    "        dd = fname_or_dict\n",
    "        \n",
    "    backwards_compatibility_replacements(dd)\n",
    "        \n",
    "    want_shapemodel = 'shapedirs' in dd\n",
    "    nposeparms = dd['kintree_table'].shape[1]*3\n",
    "\n",
    "    if 'trans' not in dd:\n",
    "        dd['trans'] = np.zeros(3)\n",
    "    if 'pose' not in dd:\n",
    "        dd['pose'] = np.zeros(nposeparms)\n",
    "    if 'shapedirs' in dd and 'betas' not in dd:\n",
    "        dd['betas'] = np.zeros(dd['shapedirs'].shape[-1])\n",
    "\n",
    "    for s in ['v_template', 'weights', 'posedirs', 'pose', 'trans', 'shapedirs', 'betas', 'J']:\n",
    "        if (s in dd) and not hasattr(dd[s], 'dterms'):\n",
    "            dd[s] = ch.array(dd[s])\n",
    "\n",
    "    if want_shapemodel:\n",
    "        dd['v_shaped'] = dd['shapedirs'].dot(dd['betas'])+dd['v_template']\n",
    "        v_shaped = dd['v_shaped']\n",
    "        J_tmpx = MatVecMult(dd['J_regressor'], v_shaped[:,0])        \n",
    "        J_tmpy = MatVecMult(dd['J_regressor'], v_shaped[:,1])        \n",
    "        J_tmpz = MatVecMult(dd['J_regressor'], v_shaped[:,2])        \n",
    "        dd['J'] = ch.vstack((J_tmpx, J_tmpy, J_tmpz)).T    \n",
    "        dd['v_posed'] = v_shaped + dd['posedirs'].dot(posemap(dd['bs_type'])(dd['pose']))\n",
    "    else:    \n",
    "        dd['v_posed'] = dd['v_template'] + dd['posedirs'].dot(posemap(dd['bs_type'])(dd['pose']))\n",
    "            \n",
    "    return dd\n",
    "\n",
    "\n",
    "\n",
    "def load_model(fname_or_dict):\n",
    "    dd = ready_arguments(fname_or_dict)\n",
    "    \n",
    "    args = {\n",
    "        'pose': dd['pose'],\n",
    "        'v': dd['v_posed'],\n",
    "        'J': dd['J'],\n",
    "        'weights': dd['weights'],\n",
    "        'kintree_table': dd['kintree_table'],\n",
    "        'xp': ch,\n",
    "        'want_Jtr': True,\n",
    "        'bs_style': dd['bs_style']\n",
    "    }\n",
    "    \n",
    "    result, Jtr = verts_core(**args)\n",
    "    result = result + dd['trans'].reshape((1,3))\n",
    "    result.J_transformed = Jtr + dd['trans'].reshape((1,3))\n",
    "\n",
    "    for k, v in dd.items():\n",
    "        setattr(result, k, v)\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "m = load_model('../smpl/models/basicmodel_m_lbs_10_207_0_v1.0.0.pkl')\n",
    "\n",
    "m.pose[:] = theta[0,3:75]\n",
    "m.betas[:] = theta[0, 75:]\n",
    "\n",
    "m.pose[0] = 0\n",
    "m.pose[1] = 0\n",
    "m.pose[2] = 0\n",
    "\n",
    "def pcd_from_smpl(m):\n",
    "    vertices = o3d.utility.Vector3dVector(m)\n",
    "    faces = o3d.utility.Vector3iVector(m.f)\n",
    "\n",
    "    pcd = o3d.geometry.TriangleMesh()\n",
    "    pcd.vertices = vertices\n",
    "    pcd.triangles = faces\n",
    "    pcd.compute_triangle_normals()\n",
    "    pcd.compute_vertex_normals()\n",
    "    \n",
    "    return pcd\n",
    "\n",
    "o3d.visualization.draw_geometries([pcd_from_smpl(m)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process whole video through this pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "results_top = []\n",
    "results_bottom = []\n",
    "\n",
    "for i in tqdm(range(0, total_frames)):\n",
    "    \n",
    "    try:\n",
    "        frame = get_frame(i, top=True)\n",
    "        crop, proc_param, img = preprocess_image(frame, get_json_path(i, True))\n",
    "        input_img = np.expand_dims(crop, 0)\n",
    "        joints, verts, cams, joints3d, theta = model.predict(input_img, get_theta=True)    \n",
    "        res_top = {'joints': joints[0], 'verts': verts[0], 'cams': cams[0], 'joints3d': joints3d[0], 'theta': theta[0,3:75], 'beta': theta[0,75:]}\n",
    "    except:\n",
    "        print('skip')\n",
    "    results_top.append(res_top)\n",
    "    \n",
    "    try:\n",
    "        frame = get_frame(i, top=False)\n",
    "        crop, proc_param, img = preprocess_image(frame, get_json_path(i, False))\n",
    "        input_img = np.expand_dims(crop, 0)\n",
    "        joints, verts, cams, joints3d, theta = model.predict(input_img, get_theta=True)\n",
    "        res_bottom = {'joints': joints[0], 'verts': verts[0], 'cams': cams[0], 'joints3d': joints3d[0], 'theta': theta[0,3:75], 'beta': theta[0,75:]}\n",
    "    except:\n",
    "        print('skip')\n",
    "    results_bottom.append(res_bottom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (f'results{filename_N}.pkl', 'wb') as handle:\n",
    "    pickle.dump([results_top, results_bottom], handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
