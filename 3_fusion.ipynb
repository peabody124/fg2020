{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: running this will require IMU data synchronized to the video\n",
    "\n",
    "# Note: while step 2 (using HMR) requires Tensorflow 1.14 this step\n",
    "# requires Tensorflow 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 32
    },
    "colab_type": "code",
    "id": "Uy_z3HuomKmW",
    "outputId": "3c6c0074-c5dd-4e66-a496-56fe4f213741"
   },
   "outputs": [],
   "source": [
    "%pylab notebook\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import cv2\n",
    "\n",
    "tfk = tf.keras\n",
    "tfkl = tfk.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a calibrated camera model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a real calibration for the camera\n",
    "cal = pickle.load(open('calibration.pkl', 'rb'))\n",
    "\n",
    "# build the camera observation viewpoints (from a prior calibration)\n",
    "global_translation = 1 * np.array([[0], [0], [0]])\n",
    "camera1_intrinsic = cal['cameraMatrix1']\n",
    "\n",
    "# \"hacking\" camera position from origin until we learn translation of pose\n",
    "camera1_extrinsic = np.concatenate([np.eye(3), global_translation], axis=1)\n",
    "\n",
    "camera2_intrinsic = cal['cameraMatrix2']\n",
    "camera2_extrinsic = np.concatenate([cal['R'], global_translation + cal['T']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from https://github.com/akanazawa/hmr/tree/master/src/tf_smpl\n",
    "# Run through tf_upgrade_v2 and then with a few more tweaks to get\n",
    "# running in eager mode\n",
    "#\n",
    "# Additional modifications to make it behave like a Tensorflow Layer\n",
    "\n",
    "def segment_names():\n",
    "    return ['hips', 'leftUpLeg', 'rightUpLeg', 'spine', 'leftLeg', 'rightLeg',\n",
    "            'spine1', 'leftFoot', 'rightFoot', 'spine2', 'leftToeBase', 'rightToeBase',\n",
    "            'neck', 'leftShoulder', 'rightShoulder', 'head', 'leftArm', 'rightArm',\n",
    "            'leftForeArm', 'rightForeArm', 'leftHand', 'rightHand', 'leftHandIndex1',\n",
    "            'rightHandIndex1']\n",
    "segment_idx = dict(zip(segment_names(), range(24)))\n",
    "\n",
    "def joint_idx(name):\n",
    "    idx = segment_idx[name]\n",
    "    return np.arange(3*idx, 3*idx+3)\n",
    "\n",
    "def batch_rodrigues(theta, name=None):\n",
    "    \"\"\"\n",
    "    Theta is N x 3\n",
    "    \n",
    "    Using code from tensorflow graphics to compute the transform\n",
    "    matrix.\n",
    "    \"\"\"\n",
    "    batch_size = tf.shape(theta)[0]\n",
    "\n",
    "    # angle = tf.norm(theta, axis=1)\n",
    "    # r = tf.expand_dims(tf.div(theta, tf.expand_dims(angle + 1e-8, -1)), -1)\n",
    "    # angle = tf.expand_dims(tf.norm(theta, axis=1) + 1e-8, -1)\n",
    "    angle = tf.norm(theta + 1e-6, axis=1, keepdims=True)\n",
    "    axis = tf.divide(theta, angle)\n",
    "\n",
    "    sin_axis = tf.sin(angle) * axis\n",
    "    cos_angle = tf.cos(angle)\n",
    "    cos1_axis = (1.0 - cos_angle) * axis\n",
    "    _, axis_y, axis_z = tf.unstack(axis, axis=-1)\n",
    "    cos1_axis_x, cos1_axis_y, _ = tf.unstack(cos1_axis, axis=-1)\n",
    "    sin_axis_x, sin_axis_y, sin_axis_z = tf.unstack(sin_axis, axis=-1)\n",
    "    tmp = cos1_axis_x * axis_y\n",
    "    m01 = tmp - sin_axis_z\n",
    "    m10 = tmp + sin_axis_z\n",
    "    tmp = cos1_axis_x * axis_z\n",
    "    m02 = tmp + sin_axis_y\n",
    "    m20 = tmp - sin_axis_y\n",
    "    tmp = cos1_axis_y * axis_z\n",
    "    m12 = tmp - sin_axis_x\n",
    "    m21 = tmp + sin_axis_x\n",
    "    diag = cos1_axis * axis + cos_angle\n",
    "    diag_x, diag_y, diag_z = tf.unstack(diag, axis=-1)\n",
    "    matrix = tf.stack((diag_x, m01, m02,\n",
    "                       m10, diag_y, m12,\n",
    "                       m20, m21, diag_z),\n",
    "                      axis=-1)  # pyformat: disable\n",
    "    output_shape = tf.concat((tf.shape(input=axis)[:-1], (3, 3)), axis=-1)\n",
    "    return tf.reshape(matrix, shape=output_shape)\n",
    "\n",
    "def batch_global_rigid_transformation(Rs, Js, parent, rotate_base=False, dtype=tf.float32):\n",
    "    \"\"\"\n",
    "    Computes absolute joint locations given pose.\n",
    "\n",
    "    rotate_base: if True, rotates the global rotation by 90 deg in x axis.\n",
    "    if False, this is the original SMPL coordinate.\n",
    "\n",
    "    Args:\n",
    "      Rs: N x 24 x 3 x 3 rotation vector of K joints\n",
    "      Js: N x 24 x 3, joint locations before posing\n",
    "      parent: 24 holding the parent id for each index\n",
    "\n",
    "    Returns\n",
    "      new_J : `Tensor`: N x 24 x 3 location of absolute joints\n",
    "      A     : `Tensor`: N x 24 4 x 4 relative joint transformations for LBS.\n",
    "    \"\"\"\n",
    "    \n",
    "    N = tf.shape(Rs)[0]\n",
    "\n",
    "    if rotate_base:\n",
    "        print('Flipping the SMPL coordinate frame!!!!')\n",
    "        rot_x = tf.constant([[1, 0, 0], [0, -1, 0], [0, 0, -1]], dtype=Rs.dtype)\n",
    "        rot_x = tf.reshape(tf.tile(rot_x, [N, 1]), [N, 3, 3])\n",
    "        root_rotation = tf.matmul(Rs[:, 0, :, :], rot_x)\n",
    "    else:\n",
    "        root_rotation = Rs[:, 0, :, :]\n",
    "\n",
    "    # Now Js is N x 24 x 3 x 1\n",
    "    Js = tf.expand_dims(Js, axis=-1)\n",
    "\n",
    "    def make_A(R, t, name=None):\n",
    "        # Rs is N x 3 x 3, ts is N x 3 x 1\n",
    "        R_homo = tf.pad(tensor=R, paddings=[[0, 0], [0, 1], [0, 0]])\n",
    "        t_homo = tf.concat([t, tf.ones([N, 1, 1], dtype=dtype)], axis=1)\n",
    "        return tf.concat([R_homo, t_homo], axis=2)\n",
    "\n",
    "    A0 = make_A(root_rotation, Js[:, 0])\n",
    "    results = [A0]\n",
    "    for i in range(1, parent.shape[0]):\n",
    "        j_here = Js[:, i] - Js[:, parent[i]]\n",
    "        A_here = make_A(Rs[:, i], j_here)\n",
    "        res_here = results[parent[i]] @ A_here\n",
    "        results.append(res_here)\n",
    "\n",
    "    # N x 24 x 4 x 4\n",
    "    results = tf.stack(results, axis=1)\n",
    "\n",
    "    new_J = results[:, :, :3, 3]\n",
    "\n",
    "    # --- Compute relative A: Skinning is based on\n",
    "    # how much the bone moved (not the final location of the bone)\n",
    "    # but (final_bone - init_bone)\n",
    "    # ---\n",
    "    Js_w0 = tf.concat([Js, tf.zeros([N, 24, 1, 1], dtype=dtype)], 2)\n",
    "    init_bone = results @ Js_w0\n",
    "    # Append empty 4 x 3:\n",
    "    init_bone = tf.pad(tensor=init_bone, paddings=[[0, 0], [0, 0], [0, 0], [3, 0]])\n",
    "    A = results - init_bone\n",
    "\n",
    "    return new_J, A\n",
    "\n",
    "# There are chumpy variables so convert them to numpy.\n",
    "def undo_chumpy(x):\n",
    "    return x if isinstance(x, np.ndarray) else x.r\n",
    "\n",
    "class SMPL(tfkl.Layer):\n",
    "    \"\"\" See paper for full description. Internal joint indices are\n",
    "    \n",
    "        0 MidHip\n",
    "        1 LHip\n",
    "        2 RHip\n",
    "        4 LKnee\n",
    "        5 RKnee\n",
    "        7 LAnkle\n",
    "        8 RAnkle\n",
    "        12 Neck\n",
    "        15 Nose\n",
    "        16 LShoulder\n",
    "        17 RShoulder\n",
    "        18 LElbow\n",
    "        19 RElbow\n",
    "        20 LWrist\n",
    "        21 RWrist\n",
    "    \"\"\"\n",
    "    def __init__(self, pkl_path='neutral_smpl_with_cocoplus_reg.pkl', \n",
    "                 joint_type='lsp', **kwargs):\n",
    "        \"\"\"\n",
    "        pkl_path is the path to a SMPL model\n",
    "        \"\"\"\n",
    "        super(SMPL, self).__init__(**kwargs)\n",
    "        \n",
    "        # -- Load SMPL params --\n",
    "        self.pkl_path = pkl_path\n",
    "        with open(pkl_path, 'rb') as f:\n",
    "            dd = pickle.load(f, encoding=\"latin-1\")\n",
    "        \n",
    "        # Mean template vertices\n",
    "        self.v_template = tf.constant(undo_chumpy(dd['v_template']), name='v_template', dtype=self.dtype)\n",
    "        \n",
    "        # Size of mesh [Number of vertices, 3]\n",
    "        self.size = tf.TensorShape([tf.shape(self.v_template)[0], 3])\n",
    "        self.num_betas = dd['shapedirs'].shape[-1]\n",
    "        # Shape blend shape basis: 6980 x 3 x 10\n",
    "        # reshaped to 6980*30 x 10, transposed to 10x6980*3\n",
    "        shapedir = np.reshape(undo_chumpy(dd['shapedirs']), [-1, self.num_betas]).T\n",
    "        self.shapedirs = tf.constant(shapedir, name='shapedirs', dtype=self.dtype)\n",
    "\n",
    "        # Regressor for joint locations given shape - 6890 x 24\n",
    "        self.J_regressor = tf.constant(dd['J_regressor'].T.todense(), name=\"J_regressor\", dtype=self.dtype)\n",
    "\n",
    "        # Pose blend shape basis: 6890 x 3 x 207, reshaped to 6890*30 x 207\n",
    "        num_pose_basis = dd['posedirs'].shape[-1]\n",
    "        # 207 x 20670\n",
    "        posedirs = np.reshape(undo_chumpy(dd['posedirs']), [-1, num_pose_basis]).T\n",
    "        self.posedirs = tf.constant(posedirs, name='posedirs', dtype=self.dtype)\n",
    "\n",
    "        # indices of parents for each joints\n",
    "        self.parents = dd['kintree_table'][0].astype(np.int32)\n",
    "\n",
    "        # LBS weights\n",
    "        self.lbs_weights = tf.constant(undo_chumpy(dd['weights']), name='lbs_weights', dtype=self.dtype)\n",
    "        \n",
    "        self.joint_type = joint_type\n",
    "        self.raw_joint_indices = tf.constant([8, 5, 2, 1, 4, 7, 21, 19, 17, 16, 18, 20, 12, 15], dtype=tf.int32)\n",
    "\n",
    "        # This returns 19 keypoints: 6890 x 19\n",
    "        self.joint_regressor = tf.constant(dd['cocoplus_regressor'].T.todense(), name=\"cocoplus_regressor\", dtype=self.dtype)\n",
    "        if joint_type == 'lsp':  # 14 LSP joints!\n",
    "            self.joint_regressor = self.joint_regressor[:, :14]\n",
    "\n",
    "    def call(self, beta, theta):\n",
    "        \"\"\"\n",
    "        Obtain SMPL with shape (beta) & pose (theta) inputs.\n",
    "        Theta includes the global rotation.\n",
    "        Args:\n",
    "          beta: N x 10\n",
    "          theta: N x 72 (with 3-D axis-angle rep)\n",
    "\n",
    "        Updates:\n",
    "          self.J_transformed: N x 24 x 3 joint location after shaping & posing with beta and theta\n",
    "          self.verts - N x 6980 x 3 vertices of body shell\n",
    "        Returns:\n",
    "          Joints: N x 19 or 14 x 3 joint locations depending on joint_type\n",
    "          Rs: N x 24 x 3 x 3 rotation matrices for each of the body segment reference frames. Note \n",
    "              that these are rotations relative to the neutral posture and not absolute segment\n",
    "              orientations.\n",
    "        \"\"\"\n",
    "\n",
    "        beta = tf.cast(tf.convert_to_tensor(beta), dtype=self.dtype)\n",
    "        theta = tf.cast(tf.convert_to_tensor(theta), dtype=self.dtype)\n",
    "        \n",
    "        num_batch = tf.shape(theta)[0]\n",
    "        num_joints = 24\n",
    "        \n",
    "        # 1. Add shape blend shapes\n",
    "        # (N x 10) x (10 x 6890*3) = N x 6890 x 3\n",
    "        v_shaped = tf.reshape(tf.reshape(beta, [1, 10]) @ self.shapedirs, [-1, self.size[0], self.size[1]]) + self.v_template\n",
    "\n",
    "        # 2. Infer shape-dependent joint locations.\n",
    "        Jx = v_shaped[:, :, 0] @ self.J_regressor\n",
    "        Jy = v_shaped[:, :, 1] @ self.J_regressor\n",
    "        Jz = v_shaped[:, :, 2] @ self.J_regressor\n",
    "        J = tf.tile(tf.stack([Jx, Jy, Jz], axis=2), [num_batch, 1, 1])\n",
    "        \n",
    "        # 3. Add pose blend shapes\n",
    "        # N x 24 x 3 x 3\n",
    "        Rs = tf.reshape(batch_rodrigues(tf.reshape(theta, [-1, 3])), [-1, num_joints, 3, 3])\n",
    "\n",
    "        #4. Get the global joint location\n",
    "        self.J_transformed, A = batch_global_rigid_transformation(Rs, J, self.parents, dtype=self.dtype)\n",
    "        if self.joint_type == 'raw':\n",
    "            return tf.gather(self.J_transformed, self.raw_joint_indices, axis=1)\n",
    "\n",
    "        # 5. Do skinning:\n",
    "        # Ignore global rotation.\n",
    "        pose_feature = tf.reshape(Rs[:, 1:, :, :] - tf.eye(3, dtype=self.dtype), [-1, 207])\n",
    "        \n",
    "        # overwrite Rs with what we actually want to return -- the absolutely (change) of reference\n",
    "        # frame for each segment\n",
    "        Rs = A[:,:,:3,:3]\n",
    "\n",
    "        # (N x 207) x (207, 20670) -> N x 6890 x 3\n",
    "        v_posed = tf.reshape(pose_feature @ self.posedirs, [-1, self.size[0], self.size[1]]) + v_shaped\n",
    "\n",
    "        # W is N x 6890 x 24\n",
    "        W = tf.reshape(tf.tile(self.lbs_weights, [num_batch, 1]), [num_batch, -1, num_joints])\n",
    "        # (N x 6890 x 24) x (N x 24 x 16)\n",
    "        A = tf.reshape(A, [num_batch, num_joints, 16])\n",
    "        T = tf.reshape(W @ A, [num_batch, -1, 4, 4])\n",
    "        v_posed_homo = tf.pad(v_posed, [[0,0], [0,0], [0,1]], 'CONSTANT', tf.constant(1.0, dtype=self.dtype))\n",
    "        \n",
    "        v_homo = T @ tf.expand_dims(v_posed_homo, axis=-1)\n",
    "    \n",
    "        verts = v_homo[:, :, :3, 0]\n",
    "\n",
    "        # Get cocoplus or lsp joints:\n",
    "        joint_x = tf.matmul(verts[:, :, 0], self.joint_regressor)\n",
    "        joint_y = tf.matmul(verts[:, :, 1], self.joint_regressor)\n",
    "        joint_z = tf.matmul(verts[:, :, 2], self.joint_regressor)\n",
    "        joints = tf.stack([joint_x, joint_y, joint_z], axis=2)\n",
    "        self.verts = verts\n",
    "        \n",
    "        # restore shape of A\n",
    "        A = tf.reshape(A, [num_batch, num_joints, 4, 4])\n",
    "        return joints, Rs\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super(SMPL, self).get_config()\n",
    "        base_config.update({'pkl_path': self.pkl_path, 'joint_type': self.joint_type})\n",
    "        return base_config\n",
    "    \n",
    "frame_id = 90\n",
    "frames = 100\n",
    "\n",
    "# for some reason this model isn't stable with large batches when using\n",
    "# float32. This comes when computing v_homo. For some reason even having\n",
    "# identical imputs for each batch element massively blows up in a way that\n",
    "# isn't clearly from a precision issue\n",
    "smpl = SMPL('neutral_smpl_with_cocoplus_reg.pkl', joint_type='lsp', dtype=tf.float32)\n",
    "coordinates_3d, Rs = smpl(np.zeros((10,), dtype=np.float32), np.zeros((frames,72), dtype=np.float32)) #pose(x)[1]) #bs(x)[1]*0\n",
    "display(np.around(coordinates_3d.numpy()[frame_id], decimals=3))\n",
    "\n",
    "#smpl = SMPL('neutral_smpl_with_cocoplus_reg.pkl', joint_type='raw', dtype=tf.float64)\n",
    "# raw: MidHip, LHip, RHip, LKnee, RKnee, LAnkle, RAnkle, Neck, Nose, LShoulder, RShoulder, LElbow, RElbow, LWrist, RWrist\n",
    "# smpl.raw_joint_indices = [0, 1, 2, 4, 5, 7, 8, 12, 15, 16, 17, 18, 19, 20, 21]\n",
    "#coordinates_raw = smpl(np.zeros((10,), dtype=np.float32), np.zeros((frames,72), dtype=np.float32)) #pose(x)[1]) #bs(x)[1]*0\n",
    "\n",
    "smpl.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple fuctional to translate and scale pose\n",
    "\n",
    "def origin_wrapper(smpl, scale, beta, origin, theta):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "     origin: N x 3\n",
    "     scale:  1\n",
    "     beta:   10\n",
    "     theta:  N x 72\n",
    "\n",
    "    Returns: N x (19 or 14) x 3 joint positions\n",
    "    \"\"\"\n",
    "\n",
    "    joints_centered, Rs = smpl(beta, theta)\n",
    "    origin = tf.expand_dims(origin, axis=1)\n",
    "    return scale * joints_centered + origin, Rs\n",
    "    \n",
    "origin_smpl = lambda scale, beta, center, theta: origin_wrapper(smpl, scale, beta, center, theta)\n",
    "\n",
    "frames = 100\n",
    "beta = np.zeros((10,), dtype=np.float32)\n",
    "theta = np.zeros((frames,72), dtype=np.float32)\n",
    "center = np.zeros((frames,3), dtype=np.float32)\n",
    "scale = 100.0\n",
    "joints, Rs = origin_smpl(scale, beta, center, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measurement and Pose model\n",
    "\n",
    "The above code establishes the forward kinematics for the transformations. This develops the layers that stores the pose state (essentially a mapping from frame number to a pose that can be backpropagated to) and the layers that project the \n",
    "$$\\mathtt{pose} \\rightarrow \\mathtt{coordinate frames} \\rightarrow \\mathtt{measurements}$$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def circular(var):\n",
    "    return tf.clip_by_value(var, -np.pi/2, np.pi/2)\n",
    "\n",
    "class SmoothL2Regularizer(tf.keras.regularizers.L1L2):\n",
    "\n",
    "    def __init__(self, smooth_fraction=0.99, **kwargs):\n",
    "        super(SmoothL2Regularizer, self).__init__(**kwargs)\n",
    "        self.smooth_fraction = smooth_fraction\n",
    "\n",
    "    def __call__(self, x):\n",
    "        dx = x[1:] - x[:-1]\n",
    "        \n",
    "        l_dx = super(SmoothL2Regularizer, self).__call__(dx)\n",
    "        l_x = super(SmoothL2Regularizer, self).__call__(x)\n",
    "        return l_dx * self.smooth_fraction + l_x * (1 - self.smooth_fraction)\n",
    "    \n",
    "    def get_config(self):\n",
    "        base_config = super(SmoothL2Regularizer, self).get_config()\n",
    "        base_config.update({'smooth_fraction': self.smooth_fraction})\n",
    "        return base_config\n",
    "\n",
    "class Pose(tfkl.Layer):\n",
    "    \"\"\" A learnable \"Pose State\" layer\n",
    "    \n",
    "        As input takes a vector of time steps and returns the pose at each time\n",
    "        based on the learned model. In this simple implementation it is just a \n",
    "        lookup table and anticipates integer time values\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, N=100, M=72, smoothness=1, pose_l2=1, **kwargs):\n",
    "        \"\"\" \"\"\"\n",
    "        super(Pose, self).__init__(**kwargs)\n",
    "\n",
    "        self.smoothness = smoothness\n",
    "        self.pose_l2 = pose_l2\n",
    "        self.pose = self.add_weight(shape=(N,M), trainable=True, dtype=tf.float32,\n",
    "                                    regularizer=SmoothL2Regularizer(smooth_fraction=smoothness, l2=pose_l2),\n",
    "                                    initializer=tf.initializers.zeros(), name='pose_estimate')\n",
    "        self.location = self.add_weight(shape=(N,3), trainable=True, dtype=tf.float32, \n",
    "                                        regularizer=SmoothL2Regularizer(smooth_fraction=1.0, l2=pose_l2),\n",
    "                                        initializer=tf.initializers.zeros(), name='location_estimate')\n",
    "\n",
    "    def call(self, time):\n",
    "         # elbow has one axis and should only go one way (note the sign for left arm versus right)\n",
    "        bad_anatomy = tf.reduce_sum(tf.square(self.pose[:, 3 * segment_idx['rightForeArm'] + 2])) + \\\n",
    "                      tf.reduce_sum(tf.square(self.pose[:, 3 * segment_idx['leftForeArm'] + 2])) +  \\\n",
    "                      tf.reduce_sum(tf.nn.relu(-self.pose[:, 3 * segment_idx['rightForeArm'] + 1])) + \\\n",
    "                      tf.reduce_sum(tf.nn.relu(self.pose[:, 3 * segment_idx['leftForeArm'] + 1]))\n",
    "        self.add_loss(1e4 * bad_anatomy)\n",
    "        return tf.gather(self.location, time), tf.gather(self.pose, time)\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super(Pose, self).get_config()\n",
    "        base_config.update({'N': self.pose.shape[0], 'M': self.pose.shape[1],\n",
    "                            'smoothness': self.smoothness, 'pose_l2': self.pose_l2})\n",
    "        return base_config\n",
    "    \n",
    "class BodyShape(tfkl.Layer):\n",
    "    \"\"\" A learnable body shape representation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, initial_shape=None, **kwargs):\n",
    "        super(BodyShape, self).__init__(**kwargs)\n",
    "        \n",
    "        if initial_shape is None:\n",
    "            initializer = tf.initializers.Constant(np.zeros((10,), dtype=np.float32))\n",
    "        else:\n",
    "            initializer = tf.initializers.Constant(initial_shape)\n",
    "        self.body_shape = self.add_weight(name='body_shape', shape=[10], dtype=tf.float32, initializer=initializer, trainable=True)\n",
    "        self.scale = self.add_weight(name='scale', shape=[], dtype=tf.float32, initializer=tf.initializers.Constant(1.0))\n",
    "        \n",
    "    def call(self, time):        \n",
    "        batch_size = tf.shape(time)[0]\n",
    "        return tf.nn.elu(self.scale), self.body_shape\n",
    "    \n",
    "    def get_config(self):\n",
    "        return super(BodyShape, self).get_config()\n",
    "    \n",
    "def get_rotation_matrix_from_quaternion(quaternion):\n",
    "    # from Tensorflow Graphics but able to map to trainable variables\n",
    "    x, y, z, w = tf.unstack(quaternion, axis=-1)\n",
    "    tx = 2.0 * x\n",
    "    ty = 2.0 * y\n",
    "    tz = 2.0 * z\n",
    "    twx = tx * w\n",
    "    twy = ty * w\n",
    "    twz = tz * w\n",
    "    txx = tx * x\n",
    "    txy = ty * x\n",
    "    txz = tz * x\n",
    "    tyy = ty * y\n",
    "    tyz = tz * y\n",
    "    tzz = tz * z\n",
    "    matrix = tf.stack((1.0 - (tyy + tzz), txy - twz, txz + twy,\n",
    "                       txy + twz, 1.0 - (txx + tzz), tyz - twx,\n",
    "                       txz - twy, tyz + twx, 1.0 - (txx + tyy)),\n",
    "                      axis=-1)  # pyformat: disable\n",
    "    output_shape = tf.concat((tf.shape(input=quaternion)[:-1], (3, 3)), axis=-1)\n",
    "    return tf.reshape(matrix, shape=output_shape)\n",
    "\n",
    "class QuaternionNormalization(tfkl.Layer):\n",
    "    \"\"\" Layer to renormalize a renormalize a unit quaternion\n",
    "    \n",
    "        Excepts to operate on data of size [..., 4] for any batch\n",
    "        size, simply normalizes on the last dimension. Does not\n",
    "        actually assert the last dimension is 4 long.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, norm_err_penalty=1e-3, **kwargs):\n",
    "        super(QuaternionNormalization, self).__init__(**kwargs)\n",
    "        self.norm_err_penalty = norm_err_penalty\n",
    "        self.epsilon = 1e-5\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        \n",
    "        # weakly penalize deviations from norm length prior to renomalization. Per\n",
    "        # Pavlo et al., 2019 this improves training\n",
    "        norm = tf.reduce_sum(tf.square(inputs), axis=-1, keepdims=True)\n",
    "        norm_err = self.norm_err_penalty * tf.reduce_sum(tf.square(1 - norm))\n",
    "        self.add_loss(norm_err)\n",
    "        \n",
    "        return inputs / (tf.sqrt(norm) + self.epsilon)\n",
    "    \n",
    "    def get_config(self):\n",
    "        base_config = super(QuaternionNormalization, self).get_config()\n",
    "        base_config.update({'norm_err_penalty': self.norm_err_penalty})\n",
    "        return base_config\n",
    "    \n",
    "class TrainableRotation(tfkl.Layer):\n",
    "    def __init__(self, N, **kwargs):\n",
    "        super(TrainableRotation, self).__init__(**kwargs)\n",
    "        self.quaternion = tf.Variable(tf.tile(tf.constant([[1.0, 0.0, 0.0, 0.0]], dtype=tf.float32) , [N, 1]),\n",
    "                                      trainable=True, name='quaternion_angles')\n",
    "        self.quaternion_normalization = QuaternionNormalization()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        normalized_quaternion = self.quaternion_normalization(self.quaternion)\n",
    "        return get_rotation_matrix_from_quaternion(normalized_quaternion)\n",
    "    \n",
    "    def get_config(self):\n",
    "        base_config = super(TrainableRotation, self).get_config()\n",
    "        base_config.update({'N': self.quaternion.shape[0]})\n",
    "        return base_config    \n",
    "    \n",
    "class PredictedImuMeasurements(tfkl.Layer):\n",
    "    \"\"\" Compute expected IMU rotations based on pose \"\"\"\n",
    "    def __init__(self, imu_frames, **kwargs):\n",
    "        super(PredictedImuMeasurements, self).__init__(**kwargs)\n",
    "        \n",
    "        self.imu_frames = tf.constant(imu_frames, dtype=tf.bool)\n",
    "        \n",
    "        # Initialize with a identity matrix (no rotation). Need to constrain\n",
    "        # degrees of freedom while training. Attempted to use the TFG rotation\n",
    "        # layers but could not build up a trainable rotation parameterization.\n",
    "        N = np.sum(imu_frames)\n",
    "        self.Rib = TrainableRotation(N, name='Rig')\n",
    "        self.Rig = TrainableRotation(N, name='Rig')\n",
    "    \n",
    "    def call(self, predicted_rotations):        \n",
    "        # Only keep the rotation frames we are measuring\n",
    "        predicted_rotations = tf.boolean_mask(predicted_rotations, self.imu_frames, axis=1)\n",
    "\n",
    "        # And apply our current observation transformation estimate\n",
    "        return self.Rig(None) @ predicted_rotations @ self.Rib(None)\n",
    "    \n",
    "    def get_config(self):\n",
    "        base_config = super(PredictedImuMeasurements, self).get_config()\n",
    "        base_config.update({'imu_frames': self.imu_frames})\n",
    "        return base_config\n",
    "    \n",
    "class PredictedImagePositions(tfkl.Layer):\n",
    "    \"\"\" Compute expected image positions rotations based on pose \"\"\"\n",
    "    def __init__(self, intrinsic_camera, extrinsic_camera, **kwargs):\n",
    "        super(PredictedImagePositions, self).__init__(**kwargs)\n",
    "        \n",
    "        self.intrinsic = tf.convert_to_tensor(intrinsic_camera, dtype=tf.float32)\n",
    "        self.extrinsic = tf.convert_to_tensor(extrinsic_camera, dtype=tf.float32)\n",
    "        self.camera = self.intrinsic @ self.extrinsic\n",
    "        #self.joint_frames = tf.constant(joint_frames, dtype=tf.bool)\n",
    "        \n",
    "    def call(self, predicted_endpoints):\n",
    "        endpoints_homogeneous = tf.pad(predicted_endpoints, tf.constant([[0, 0], [0, 0], [0, 1]]), 'CONSTANT', tf.constant(1.0))\n",
    "        \n",
    "        # Only keep the rotation frames we are measuring\n",
    "        #endpoints_homogeneous = tf.boolean_mask(endpoints_homogeneous, self.joint_frames, axis=1)\n",
    "        predicted_image_homogeneous = tf.tensordot(endpoints_homogeneous, self.camera, (-1, -1))\n",
    "        \n",
    "        predicted_uv = predicted_image_homogeneous[..., :-1] / predicted_image_homogeneous[..., -1, None]\n",
    "        return predicted_uv\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super(PredictedImagePositions, self).get_config()\n",
    "        base_config.update({'intrinsic': self.intrinsic, 'extrinsic': self.extrinsic})\n",
    "        return base_config\n",
    "    \n",
    "def angular_err(y_true, y_pred):\n",
    "    from tensorflow.python.framework import ops\n",
    "    from tensorflow.python.keras import backend as K\n",
    "    from tensorflow.python.ops import math_ops\n",
    "\n",
    "    y_pred = ops.convert_to_tensor(y_pred)\n",
    "    y_true = math_ops.cast(y_true, y_pred.dtype)\n",
    "    \n",
    "    angle_difference = tf.linalg.matrix_transpose(y_true) @ y_pred\n",
    "    difference_trace = tf.linalg.trace(angle_difference)\n",
    "    angle_loss = tf.reduce_mean(tf.math.acos((difference_trace - 1) / 2.0 * (1-1e-8)))\n",
    "    return angle_loss\n",
    "\n",
    "def euclidean_err(y_true, y_pred):\n",
    "    delta = y_true - y_pred\n",
    "    dist = tf.reduce_sum(tf.square(y_true - y_pred), axis=-1)\n",
    "    return tf.reduce_mean(dist)\n",
    "\n",
    "#angular_err(R.from_euler('xyz', [0, 0, 45], degrees=True).as_dcm(), \n",
    "#            R.from_euler('xyz', [0, 40, 0], degrees=True).as_dcm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tools for working with 3D projections\n",
    "\n",
    "def stereo_triangulate(keypoints):\n",
    "    parts = keypoints.columns.get_level_values(1).unique()\n",
    "    p3d = []\n",
    "\n",
    "    smoothed = keypoints.ewm(10).mean()\n",
    "\n",
    "    for p in parts:\n",
    "        p1 = smoothed['camL'][p][['x', 'y']].values\n",
    "        p2 = smoothed['camR'][p][['x', 'y']].values\n",
    "\n",
    "        P1 = camera1_intrinsic @ camera1_extrinsic #(cal['cameraMatrix1'] @ np.concatenate((np.eye(3), np.zeros((3,1))), axis=1))\n",
    "        P2 = camera2_intrinsic @ camera2_extrinsic #(cal['cameraMatrix1'] @ np.concatenate((cal['R'], cal['T']), axis=1))\n",
    "\n",
    "        # compute 3d homogeneous\n",
    "        p3 = cv2.triangulatePoints(P1, P2, p1.transpose(), p2.transpose()).transpose()\n",
    "        p3 = cv2.convertPointsFromHomogeneous(p3)[:,0,:]\n",
    "        \n",
    "        p3 = p3[:, [0, 1, 2]]\n",
    "        p3d.append(p3)\n",
    "\n",
    "    p3d = np.stack(p3d, axis=1)\n",
    "\n",
    "    columns = pd.MultiIndex.from_product([parts, ['x', 'y', 'z']], names=['part', 'component'])\n",
    "    return pd.DataFrame(p3d.reshape((len(stereo_keypoints), -1)),  columns=columns, index=keypoints.index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform this with torso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stereo_keypoints = pd.read_pickle(f'movement.pkl')\n",
    "\n",
    "lsp_parts = [\"RAnkle\", \"RKnee\", \"RHip\", \"LHip\", \"LKnee\", \"LAnkle\", \"RWrist\", \"RElbow\", \n",
    "             \"RShoulder\", \"LShoulder\", \"LElbow\", \"LWrist\", \"Neck\", \"Nose\"]\n",
    "\n",
    "cams = stereo_keypoints.columns.get_level_values(0).unique()\n",
    "#display(stereo_keypoints[cams[0]][parts])\n",
    "\n",
    "measured_positions = []\n",
    "cals = [{'K': cal['cameraMatrix1'], 'D': cal['distCoeffs1'], 'R': cal['R1'], 'P': cal['P1']},\n",
    "       {'K': cal['cameraMatrix2'], 'D': cal['distCoeffs2'], 'R': cal['R2'], 'P': cal['P2']}]\n",
    "for c, cam_cal in zip(cams, cals):\n",
    "    for p in lsp_parts:\n",
    "        points = stereo_keypoints[c][p][['x', 'y', 'c']]\n",
    "        \n",
    "        # optionally can apply additional correction for camera distortion. reprojection to\n",
    "        # image will (correctly) look off after this\n",
    "        if False:  \n",
    "            # need to look if there is a way to factor the higher level distortions into\n",
    "            # the observation model. probably not worth it.\n",
    "            points = points.values.reshape((-1,1,2)).astype(np.float32)\n",
    "            points = cv2.undistortPoints(points, cam_cal['K'], cam_cal['D'], R=cam_cal['R'], P=cam_cal['P'])[:,0,:]\n",
    "        \n",
    "        measured_positions.append(points)\n",
    "        \n",
    "measured_positions = np.stack(measured_positions, axis=1)\n",
    "\n",
    "# need to reorder the axes as this code expects x, y, z, w\n",
    "reorder = [1, 2, 3, 0]\n",
    "imu_readings_r_forearm = pd.read_pickle(f'attitude_r_forearm{run_number}.pkl')\n",
    "imu_readings_r_arm = pd.read_pickle(f'attitude_r_arm{run_number}.pkl')\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "measured_orientations_r_forearm = R.from_quat(imu_readings_r_forearm.values[:, reorder]).as_dcm()[:, None, ...]\n",
    "measured_orientations_r_arm = R.from_quat(imu_readings_r_arm.values[:, reorder]).as_dcm()[:, None, ...]\n",
    "\n",
    "measured_orientations = np.concatenate((measured_orientations_r_arm, measured_orientations_r_forearm), axis=1)\n",
    "\n",
    "stop = 900\n",
    "stereo_keypoints = stereo_keypoints[:stop]\n",
    "measured_positions = measured_positions[:stop]\n",
    "measured_orientations = measured_orientations[:stop]\n",
    "measured_orientations_r_forearm = measured_orientations_r_forearm[:stop]\n",
    "measured_orientations_r_arm = measured_orientations_r_arm[:stop]\n",
    "imu_readings_r_forearm = imu_readings_r_forearm[:stop]\n",
    "imu_readings_r_arm = imu_readings_r_arm[:stop]\n",
    "\n",
    "# when things are missing I flagged as nan. replace with no confidence\n",
    "measured_positions[np.isnan(measured_positions)] = 0\n",
    "\n",
    "#measured_positions = measured_positions[:250:5]\n",
    "print(measured_positions.shape)\n",
    "print(measured_orientations.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,1,sharex=True)\n",
    "\n",
    "confidence_mean = np.mean(np.stack([stereo_keypoints['camL'][p]['c'].values for p in lsp_parts], axis=1), axis=1)\n",
    "confidence_good = np.logical_and(confidence_mean > 0.4, confidence_mean < 5)\n",
    "\n",
    "valid_positions = np.all(np.all(~np.isnan(measured_positions), axis=1), axis=1)\n",
    "\n",
    "keep = np.logical_and(confidence_good, valid_positions)\n",
    "\n",
    "ax[0].plot(np.where(keep)[0], np.stack([stereo_keypoints['camL'][p]['c'].values for p in lsp_parts], axis=1)[keep], '.')\n",
    "ax[0].plot(np.where(keep)[0], np.mean(np.stack([stereo_keypoints['camL'][p]['c'].values for p in lsp_parts], axis=1), axis=1)[keep], 'k')\n",
    "\n",
    "ax[1].plot(np.where(keep)[0], measured_positions[keep, 0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(N, x):\n",
    "    smpl = SMPL('neutral_smpl_with_cocoplus_reg.pkl', joint_type='lsp')\n",
    "    origin_smpl = lambda scale, beta, center, theta: origin_wrapper(smpl, scale, beta, center, theta)\n",
    "\n",
    "    bs = BodyShape(np.zeros((10,)))\n",
    "    pose = Pose(N, 72, pose_l2=2e-3, smoothness=0.99)\n",
    "\n",
    "    pred_image1 = PredictedImagePositions(camera1_intrinsic, camera1_extrinsic)\n",
    "    pred_image2 = PredictedImagePositions(camera2_intrinsic, camera2_extrinsic)\n",
    "    pred_images = [pred_image1, pred_image2]\n",
    "    \n",
    "    selected_frames = np.array([False]*24)\n",
    "    selected_frames[[17, 19]] = True\n",
    "\n",
    "    pred_imu = PredictedImuMeasurements(tf.constant(selected_frames))\n",
    "    \n",
    "    joints, Rs = origin_smpl(*bs(x), *pose(x))\n",
    "    \n",
    "    # use N cameras\n",
    "    im = tf.concat([pred(joints) for pred in pred_images], axis=1)\n",
    "    imu = pred_imu(Rs)\n",
    "    \n",
    "    return im, imu, pose, bs, pred_images, pred_imu\n",
    "\n",
    "N = len(measured_positions)\n",
    "x = np.arange(N)\n",
    "y = [measured_positions, measured_orientations]\n",
    "\n",
    "#mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "#with mirrored_strategy.scope():\n",
    "huber = tf.keras.losses.Huber()\n",
    "def position_loss(y_true, y_pred):\n",
    "    weight = y_true[..., 2]\n",
    "    y_true = y_true[..., :2]\n",
    "    return huber(y_true, y_pred, sample_weight=weight)\n",
    "\n",
    "if True:\n",
    "    # build estimator\n",
    "    model_x = tfk.Input([], dtype=tf.int32)\n",
    "    im, imu, pose, bs, pred_images, pred_imu = build_model(N, model_x)\n",
    "    obs_model = tfk.Model(inputs=model_x, outputs=[im, imu])\n",
    "    obs_model.output_names = ['Image', 'IMU']\n",
    "    learning_rate = 0.001\n",
    "    obs_model.compile(tf.optimizers.Adam(learning_rate), \n",
    "                      loss=[position_loss, lambda a, b : 0.5 * angular_err(a, b)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize from ballpark starting position\n",
    "\n",
    "triangulated_coordinates = stereo_triangulate(stereo_keypoints)\n",
    "torso_base = triangulated_coordinates['MidHip']\n",
    "mean_position = np.median(torso_base[keep], axis=0)\n",
    "print(torso_base.shape)\n",
    "print(pose.location.shape)\n",
    "pose.location.assign(torso_base.values)\n",
    "\n",
    "#for idx in np.where(~keep)[0]:\n",
    "#    pose.location[idx,:].assign(mean_position.astype(np.float32))\n",
    "#tf.tile(tf.reshape(mean_position.astype(np.float32), [1, -1]), [sum(~keep), 1])\n",
    "bs.scale.assign(100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = 100\n",
    "\n",
    "results = pickle.load(open(f'results{run_number}.pkl', 'rb'))\n",
    "theta = np.stack([r['theta'] for r in results[0]], axis=0)[offset:offset+stop]\n",
    "#theta[:,0] = theta[:,0] + np.pi/2\n",
    "beta = np.stack([r['beta'] for r in results[0]], axis=0)[offset:offset+stop]\n",
    "\n",
    "mean_beta = np.mean(beta, axis=0)\n",
    "bs.body_shape.assign(mean_beta)\n",
    "pose.pose.assign(theta);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resulting_mid_hip = np.mean(origin_smpl(*bs(x), *pose(x))[0][:,2:4,:], axis=1)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(resulting_mid_hip)\n",
    "plt.plot(torso_base.values)\n",
    "\n",
    "#offset = np.mean(torso_base.values[:900]-resulting_mid_hip[:900], axis=0, keepdims=True)\n",
    "offset = np.mean(torso_base.values - resulting_mid_hip, axis=0, keepdims=True)\n",
    "pose.location.assign(pose.location + offset)\n",
    "\n",
    "print(f'Corrected offset: {offset}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fast_epochs = 5\n",
    "fast_speed = 0.05\n",
    "lr_decay_cb = tf.keras.callbacks.LearningRateScheduler(\n",
    "   lambda epoch: fast_speed if epoch < fast_epochs else learning_rate + (fast_speed - learning_rate) * (0.99 ** (epoch-fast_epochs)),\n",
    "   verbose=False)\n",
    "\n",
    "es_cb = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=50)\n",
    "\n",
    "batch_size = len(x) #int(len(x) / 10)\n",
    "history = obs_model.fit(x, y, epochs=10000, verbose=1, batch_size=batch_size, callbacks=[es_cb]) #, lr_decay_cb]) #,  lr_decay_cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute residual error\n",
    "y_out = obs_model(x)\n",
    "\n",
    "t_offset = stereo_keypoints.index.copy()\n",
    "t_offset = t_offset - t_offset[0]\n",
    "t_offset = t_offset.total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,1,figsize=(7,7))\n",
    "sca(ax[0])\n",
    "reprojection_error = np.linalg.norm(y[0][:, :, :2] - y_out[0].numpy(), axis=2)\n",
    "plt.plot(t_offset, reprojection_error, '.', MarkerSize=1)\n",
    "plt.plot(t_offset, np.mean(reprojection_error, axis=1), 'k.')\n",
    "plt.ylabel('Reprojection error (2D pixels)')\n",
    "plt.ylim(0,50)\n",
    "\n",
    "sca(ax[1])\n",
    "\n",
    "#angular_err(y[1], y_out[1])\n",
    "median_angular_error = []\n",
    "for imu_idx in range(2):\n",
    "    rotated_imu = R.from_dcm(y[1][:, imu_idx, ...])\n",
    "    body_segment = R.from_dcm(y_out[1][:, imu_idx, ...])\n",
    "    rotation_err = rotated_imu * body_segment.inv()\n",
    "    rotation_err = np.linalg.norm(rotation_err.as_rotvec(), axis=1)\n",
    "\n",
    "    median_angular_error.append(np.median(rotation_err) * 180 / np.pi)\n",
    "    plt.plot(t_offset, rotation_err * 180 / np.pi)\n",
    "plt.ylim(0, 90)\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Rotation error (deg)')\n",
    "\n",
    "\n",
    "print(f'2D reprojection error: {np.median(reprojection_error)} pixel')\n",
    "print(f'IMU angular error: {median_angular_error} deg')\n",
    "\n",
    "\n",
    "#y[0][:, :, :2].shape\n",
    "y_out[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refit now only using IMUs\n",
    "\n",
    "Use the IMU calibration from the prior run and the base pose and location (since we don't track torso current) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RestrictedPose(tfkl.Layer):\n",
    "    \"\"\" A learnable \"Pose State\" layer\n",
    "    \n",
    "        As input takes a vector of time steps and returns the pose at each time\n",
    "        based on the learned model. In this simple implementation it is just a \n",
    "        lookup table and anticipates integer time values\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, N=100, M=72, smoothness=1, pose_l2=1, **kwargs):\n",
    "        \"\"\" \"\"\"\n",
    "        super(RestrictedPose, self).__init__(**kwargs)\n",
    "\n",
    "        self.smoothness = smoothness\n",
    "        self.pose_l2 = pose_l2\n",
    "        self.pose = self.add_weight(shape=(N,M), trainable=True, dtype=tf.float32,\n",
    "                                    regularizer=SmoothL2Regularizer(smooth_fraction=smoothness, l2=pose_l2),\n",
    "                                    initializer=tf.initializers.zeros(), name='pose_estimate')\n",
    "        self.location = self.add_weight(shape=(N,3), trainable=True, dtype=tf.float32, \n",
    "                                        regularizer=SmoothL2Regularizer(smooth_fraction=1.0, l2=pose_l2),\n",
    "                                        initializer=tf.initializers.zeros(), name='location_estimate')\n",
    "\n",
    "    def call(self, time):\n",
    "         # elbow has one axis and should only go one way (note the sign for left arm versus right)\n",
    "        bad_anatomy = tf.reduce_sum(tf.square(self.pose[:, 3 * segment_idx['rightForeArm'] + 2])) + \\\n",
    "                      tf.reduce_sum(tf.square(self.pose[:, 3 * segment_idx['leftForeArm'] + 2])) +  \\\n",
    "                      tf.reduce_sum(tf.nn.relu(-self.pose[:, 3 * segment_idx['rightForeArm'] + 1])) + \\\n",
    "                      tf.reduce_sum(tf.nn.relu(self.pose[:, 3 * segment_idx['leftForeArm'] + 1]))\n",
    "        self.add_loss(1e4 * bad_anatomy)\n",
    "        \n",
    "        stiff_body_joints = self.pose[:, 1:segment_idx['rightShoulder']*3]\n",
    "        self.add_loss(tf.reduce_sum(tf.square(stiff_body_joints)))\n",
    "        \n",
    "        return tf.gather(self.location, time), tf.gather(self.pose, time)\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super(Pose, self).get_config()\n",
    "        base_config.update({'N': self.pose.shape[0], 'M': self.pose.shape[1],\n",
    "                            'smoothness': self.smoothness, 'pose_l2': self.pose_l2})\n",
    "        return base_config\n",
    "\n",
    "\n",
    "def build_imu_pose_model(N, x, bs, pred_imu):\n",
    "    smpl = SMPL('neutral_smpl_with_cocoplus_reg.pkl', joint_type='lsp')\n",
    "    origin_smpl = lambda scale, beta, center, theta: origin_wrapper(smpl, scale, beta, center, theta)\n",
    "\n",
    "    pose = RestrictedPose(N, 72, pose_l2=1e-4)\n",
    "    \n",
    "    joints, Rs = origin_smpl(*bs(x), *pose(x))\n",
    "    imu = pred_imu(Rs)\n",
    "    \n",
    "    return imu, pose\n",
    "\n",
    "pred_imu.trainable = False\n",
    "bs.trainable = False\n",
    "\n",
    "imu2, pose2 = build_imu_pose_model(N, model_x, bs, pred_imu)\n",
    "\n",
    "pose2.pose[:,:3].assign(pose.pose[:,:3])\n",
    "pose2.location.assign(pose.location)\n",
    "\n",
    "obs_model2 = tfk.Model(inputs=model_x, outputs=imu2)\n",
    "learning_rate = 0.001\n",
    "\n",
    "fast_epochs = 0\n",
    "fast_speed = 0.01\n",
    "lr_decay_cb = tf.keras.callbacks.LearningRateScheduler(\n",
    "   lambda epoch: fast_speed if epoch < fast_epochs else learning_rate + (fast_speed - learning_rate) * (0.9 ** (epoch-fast_epochs)),\n",
    "   verbose=False)\n",
    "\n",
    "\n",
    "obs_model2.compile(tf.optimizers.Adam(learning_rate), loss=angular_err)\n",
    "history2 = obs_model2.fit(x, y[1], epochs=25000, verbose=1, batch_size=batch_size, callbacks=[es_cb,  lr_decay_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())\n",
    "plt.figure()\n",
    "#plt.semilogy(history.history['loss'])\n",
    "plt.semilogy(history.history['loss'])\n",
    "plt.semilogy(history.history['Image_loss'])\n",
    "plt.title(f\"Final loss {history.history['loss'][-1]} and Eucllidean: {history.history['Image_loss'][-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the quality of position reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_pose = pose\n",
    "#selected_pose = pose2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_positions = pred_images[0](origin_smpl(*bs(x), *selected_pose(x))[0]).numpy()\n",
    "\n",
    "fig, ax = plt.subplots(3,1,sharex=True)\n",
    "ax[0].plot(image_positions[:,0,:], 'blue', label='reconstruction')\n",
    "ax[0].plot(y[0][:,0,:], 'green', label='image keypoints')\n",
    "ax[0].set_title('Waist')\n",
    "\n",
    "ax[1].plot(image_positions[:,1,:], 'blue', label='reconstruction')\n",
    "ax[1].plot(y[0][:,1,:], 'green', label='image keypoints')\n",
    "ax[1].set_title('Neck')\n",
    "\n",
    "ax[2].plot(image_positions[:,4,:], 'blue', label='reconstruction')\n",
    "ax[2].plot(y[0][:,4,:], 'green', label='image keypoints')\n",
    "ax[2].set_title('RWrist')\n",
    "\n",
    "ax[0].legend().set_visible(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, rotations = origin_smpl(*bs(x), *selected_pose(x))\n",
    "\n",
    "predicted_rotations = pred_imu(rotations)\n",
    "\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "predicted_q_arm = R.from_dcm(predicted_rotations[:,0,...])\n",
    "\n",
    "measured_arm_from_dcm = R.from_dcm(measured_orientations_r_arm[:,0,...])\n",
    "measured_forearm_from_dcm = R.from_dcm(measured_orientations_r_forearm[:,0,...])\n",
    "\n",
    "fig, ax = plt.subplots(3, 2, sharex=True, figsize=(10,8))\n",
    "ax[0,0].plot(predicted_q_arm.as_quat())\n",
    "ax[0,0].set_title('Predicted (kinematics)')\n",
    "ax[1,0].plot(measured_arm_from_dcm.as_quat())\n",
    "#ax[1,0].plot(predicted_q_arm.as_quat())\n",
    "ax[1,0].set_title('Sensor')\n",
    "\n",
    "measured_q = R.from_quat(imu_readings_r_arm.values[:, reorder])\n",
    "delta_q = np.linalg.norm((predicted_q_arm.inv() * measured_q).as_rotvec(), axis=1)\n",
    "ax[2,0].plot(delta_q)\n",
    "ax[2,0].set_ylim(0,np.pi)\n",
    "ax[2,0].set_title('Angular Difference')\n",
    "ax[2,0].set_ylabel('Rad')\n",
    "\n",
    "predicted_q_forearm = R.from_dcm(predicted_rotations[:,1,...])\n",
    "ax[0,1].plot(predicted_q_forearm.as_quat())\n",
    "ax[0,1].set_title('Predicted (kinematics)')\n",
    "ax[1,1].plot(measured_forearm_from_dcm.as_quat())\n",
    "ax[1,1].set_title('Sensor')\n",
    "\n",
    "measured_q = R.from_quat(imu_readings_r_forearm.values[:, reorder])\n",
    "delta_q = np.linalg.norm((predicted_q_forearm.inv() * measured_q).as_rotvec(), axis=1)\n",
    "ax[2,1].plot(delta_q)\n",
    "ax[2,1].set_ylim(0,np.pi)\n",
    "ax[2,1].set_title('Angular Difference')\n",
    "ax[2,1].set_ylabel('Rad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to pertinent joint angles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_joints(named_rotations):\n",
    "    def get_r_shoulder():\n",
    "        \"\"\" Recover biomechanical movements from arm versus sternum orientation\n",
    "\n",
    "            X axis is flexion\n",
    "            Y axis is IR\n",
    "            Z axis is abduction\n",
    "        \"\"\"\n",
    "        r_shoulder = named_rotations['neck'].inv() * named_rotations['rightArm']\n",
    "\n",
    "        r_shoulder_zero = R.from_euler('XYZ', [0, 0, 90], degrees=True)\n",
    "        \n",
    "        # this returns the plane of rotation as the first parameter, the elevation\n",
    "        # as the second and finally internal rotaton (after adding to the plane of\n",
    "        # rotation second). The offset is because the first rotation is really an\n",
    "        # extrinsic one so we have yXY but this library won't decompose that. Also\n",
    "        # zero for the plane of rotation comes out as backward extensions so adjust\n",
    "        # so that zero is lateral\n",
    "        r_shoulder = (r_shoulder * r_shoulder_zero.inv()).as_euler('YXY', degrees=True)\n",
    "        r_shoulder = [r_shoulder[0] - 90, r_shoulder[1], r_shoulder[2] + r_shoulder[0]]\n",
    "        #print(r_shoulder)\n",
    "        return dict(zip(['RShoulderPlane', 'RShoulderElevation', 'RShoulderIR'], r_shoulder))\n",
    "    \n",
    "    def get_r_elbow_flexion():\n",
    "        \"\"\" Recover elbow flexion as forearm versus arm\n",
    "\n",
    "            X axis is flexion\n",
    "            Y axis is IR but is passed on since we need to account for wrist\n",
    "            Z painful\n",
    "        \"\"\"\n",
    "        r_elbow = named_rotations['rightArm'].inv() * named_rotations['rightForeArm'] \n",
    "        signs = np.array([1, 1, 1])\n",
    "        r_elbow = r_elbow.as_euler('YXZ', degrees=True) * signs\n",
    "        #print(r_elbow)\n",
    "        return {'RElbowFlexion': r_elbow[0]}\n",
    "\n",
    "    def get_r_elbow_supination():\n",
    "        \"\"\" Recover forearm supination as axial rotation of hand versus arm\n",
    "\n",
    "            X axis is flexion\n",
    "            Y axis is IR but is passed on since we need to account for wrist\n",
    "            Z painful\n",
    "        \"\"\"\n",
    "        r_elbow_wrist = named_rotations['rightArm'].inv() * named_rotations['rightHand'] \n",
    "        signs = np.array([1, 1, 1])\n",
    "        r_elbow_wrist = r_elbow_wrist.as_euler('yzx', degrees=True) * signs\n",
    "        #print(r_elbow_wrist)\n",
    "        return {'RForearmSupination': r_elbow_wrist[-1]}\n",
    "\n",
    "    joints = dict()\n",
    "    joints.update(get_r_shoulder())\n",
    "    joints.update(get_r_elbow_flexion())\n",
    "    joints.update(get_r_elbow_supination())\n",
    "    return joints\n",
    "\n",
    "joints = [get_joints(dict(zip(segment_names(), R.from_dcm(time_Rs)))) for time_Rs in Rs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joints = pd.DataFrame(joints, index=t_offset)\n",
    "order = [4, 2, 3, 0, 1]\n",
    "df_joints = df_joints[df_joints.columns[order]]\n",
    "better_names = ['Shoulder Plane', 'Elevation', 'Internal Rotation', 'Elbow Flexion', 'Supination']\n",
    "df_joints = pd.DataFrame(np.degrees(np.unwrap(np.radians(df_joints.values), axis=0)), columns=better_names, index=df_joints.index)\n",
    "ax = df_joints.plot(subplots=True, figsize=(10,6))\n",
    "for a in ax:\n",
    "    a.set_ylabel('Degrees')\n",
    "    a.legend().set_frame_on(False)\n",
    "plt.xlabel('Time (s)')\n",
    "\n",
    "fig = gcf()\n",
    "fig.savefig('kinematic_traces.png', transparent=True, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare video and IMU fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3,1,figsize=(10,6), sharex=True)\n",
    "\n",
    "Rs = smpl(np.zeros((1,10)), pose.pose)[1]\n",
    "joints = [get_joints(dict(zip(segment_names(), R.from_dcm(time_Rs)))) for time_Rs in Rs]\n",
    "df_joints = pd.DataFrame(joints, index=stereo_keypoints.index)\n",
    "\n",
    "df_joints['RShoulderElevation'].plot(ax=ax[0], label='Video')\n",
    "#ax[0].set_ylim(0, 180)\n",
    "ax[0].set_title('R Shoulder Elevation')\n",
    "\n",
    "df_joints['RElbowFlexion'].plot(ax=ax[1], label='Video')\n",
    "#ax[0].set_ylim(0, 180)\n",
    "ax[1].set_title('R Elbow Flexion')\n",
    "\n",
    "df_joints['RForearmSupination'].plot(ax=ax[2], label='Video')\n",
    "#ax[0,1].set_ylim(0, 180)\n",
    "ax[2].set_title('R Forearm Supination')\n",
    "\n",
    "Rs = smpl(np.zeros((1,10)), pose2.pose)[1]\n",
    "joints = [get_joints(dict(zip(segment_names(), R.from_dcm(time_Rs)))) for time_Rs in Rs]\n",
    "df_joints_imu = pd.DataFrame(joints, index=stereo_keypoints.index)\n",
    "\n",
    "df_joints_imu['RShoulderElevation'].plot(ax=ax[0], label='IMU')\n",
    "df_joints_imu['RElbowFlexion'].plot(ax=ax[1], label='IMU')\n",
    "df_joints_imu['RForearmSupination'].plot(ax=ax[2], label='IMU')\n",
    "\n",
    "ax[1].set_ylabel('Time')\n",
    "ax[1].legend().set_visible(True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Kinematics-TF-20190919.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
